# Leaderboard
| Model Name | Score | Date Tested |
|------------|-------|-------------|
| gpt-4-1106-preview | 83.84% | 2024-01-02 |
| gpt-4 | 78.79% | 2024-01-02 |
| Mixtral-8x7B-Instruct-v0.1 | 75.76% | 2024-01-02 |
| fbn-norm | 74.75% | 2024-01-02 |
| Nous-Hermes-2-Yi-34B | 74.75% | 2024-01-03 |
| gpt-3.5-turbo | 73.74% | 2024-01-02 |
| OpenHermes-2p5-Mistral-7B | 65.66% | 2024-01-02 |
| MythoMax-L2-13b | 50.51% | 2024-01-03 |



# What is this?
We are benchmarking the ability for different models to give correct answers to Agronomy questions. This is a simple, 98 multiple-choice question benchmark today, and I plan to make it more complete and challenging in the future.

# Why?
When building new models for agriculture, it's important to know if your model is getting better or worse. This is a simple benchmark to help us determine if we are improving the agronomic ability of new models and by how much.

# Roadmap
1. Make it harder! These are fairly basic questions. We should add short and long answer questions (to be evaluated against example correct answers)
2. Add questions for international regions
3. Add more models to the leaderboard
